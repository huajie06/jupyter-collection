{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# db_url = '/Users/huajiezhang/test/jupyter-collection/news-collection.db'\n",
    "\n",
    "# db = sqlite3.connect(db_url)\n",
    "from datetime import datetime, timedelta\n",
    "dt_f = datetime.now() - timedelta(hours=48)\n",
    "f = dt_f.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "df = pd.read_sql_query(\"SELECT news_title FROM news\" \n",
    "                       \"  where news_grab_ts > '{}'\"\n",
    "                       \" and news_source <> 'Bloomberg'\".format(f), db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-07 20:35:51.812800'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dt_f = datetime.now() - timedelta(hours=2)\n",
    "# f = dt_f.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "puncs = string.punctuation\n",
    "maps = str.maketrans('','', puncs)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = [i.translate(maps) for i in stop_words]\n",
    "import json\n",
    "\n",
    "n_grms = ['south china morning post', 'hong kong', 'new york time', 'new york', 'stock market',\n",
    "         'social security', 'credit card', 'hedge fund', 'wall street', 'jp morgan',\n",
    "         'mutual fund']\n",
    "\n",
    "def string_sub(s):\n",
    "    for i in n_grms:\n",
    "        s = re.sub(i, i.translate({ord(c): None for c in string.whitespace}), s)\n",
    "    return s\n",
    "\n",
    "not_need_in_results = ['south china morning post', 'bloomberg', 'new york time', 'bloomberg daybreak', 'stock',\n",
    "                      'price','daybreak', 'wall street', 'buy', 'year', 'share', 'market', 'could', 'investor',\n",
    "                      'show', 'stock market', 'company', 'today', 'analyst', 'say', 'top', 'target', 'time',\n",
    "                      'week','new',]\n",
    "not_need_in_results_c = [i.translate({ord(c): None for c in string.whitespace}) for i in not_need_in_results]\n",
    "\n",
    "\n",
    "long_text = ' '.join(df['news_title'])\n",
    "text = unidecode.unidecode(long_text)\n",
    "text = text.translate(maps).lower()\n",
    "wd_token = nltk.word_tokenize(text)\n",
    "wd_token = [lemm.lemmatize(i) for i in wd_token]\n",
    "wd_token = [i for i in wd_token if i.isalpha()]\n",
    "long_string = ' '.join(wd_token)\n",
    "long_string = string_sub(long_string)\n",
    "wd_token = [i for i in nltk.word_tokenize(long_string) if i not in stop_words]\n",
    "\n",
    "a = nltk.FreqDist(wd_token).most_common(300)\n",
    "results = {i[0]:i[1] for i in a if not len(i[0])==1 if i[0] not in not_need_in_results_c}\n",
    "\n",
    "with open('../data.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps\n",
    "given a long string\n",
    "- rm puncs, string level\n",
    "- lower case, string level\n",
    "- stem first list level \n",
    "- rm numbers, list level\n",
    "\n",
    "- substitue terms, string level\n",
    "\n",
    "- rm stop words, list level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = [i.translate(maps) for i in stop_words]\n",
    "# nltk_text = nltk.Text(wd_token)\n",
    "nltk_text.concordance('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(dout, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json') as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_freq_to_df(nltk_freq):\n",
    "    d = {' '.join(k):[nltk_freq.get(k)] for k in nltk_freq}\n",
    "    wd_df = pd.DataFrame.from_dict(d).transpose()\n",
    "    wd_df = wd_df.reset_index()\n",
    "    wd_df.columns = ['words', 'freq']\n",
    "    wd_df = wd_df.sort_values(by='freq',ascending=False)\n",
    "    \n",
    "    return wd_df\n",
    "\n",
    "u_df = nltk_freq_to_df(uni_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
